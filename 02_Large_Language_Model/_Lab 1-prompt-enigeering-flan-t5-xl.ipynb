{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundation Model Prompt Engineering: Text2Text Generation\n",
    "Foundation Models can also be deployed using code.  You do not have to manually deploy it via the Jumpstart console.  \n",
    "\n",
    "In this lab we will deploy the text generation Foundation Model **FLAN-T5 XL** from within our code.  \n",
    "\n",
    "You will get better results from the larger Flan models, such as the **FLAN-T5 XXL FP16** however this requires more compute than we have available in our lab environment.\n",
    "\n",
    "<br>\n",
    "Execute the below code to deploy the **FLAN-T5 XL** model to an endpoint and to test its capability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import script_uris\n",
    "from sagemaker import image_uris \n",
    "from sagemaker import model_uris\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f'Using sagemaker=={sagemaker.__version__}')\n",
    "logger.info(f'Using boto3=={boto3.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the endpoint in code using the Model ID to identify the model we will deploy.  We use a '*' as the model version to ensure we are getting the latest version from the repository.  The Instance Type 'ml.p3.2xlarge' represents the compute this model will use once deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'huggingface-text2text-flan-t5-xl'  # this is hard-coded\n",
    "MODEL_VERSION = '*'\n",
    "INSTANCE_TYPE = 'ml.p3.2xlarge'\n",
    "INSTANCE_COUNT = 1\n",
    "IMAGE_SCOPE = 'inference'\n",
    "MODEL_DATA_DOWNLOAD_TIMEOUT = 3600  # in seconds\n",
    "CONTAINER_STARTUP_HEALTH_CHECK_TIMEOUT = 3600\n",
    "EBS_VOLUME_SIZE = 256  # in GB\n",
    "CONTENT_TYPE = 'application/json'\n",
    "\n",
    "# set up roles and clients \n",
    "client = boto3.client('sagemaker-runtime')\n",
    "ROLE = get_execution_role()\n",
    "logger.info(f'Role => {ROLE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unix_time = int(time.time())\n",
    "endpoint_name = f'{MODEL_ID}-{unix_time}'\n",
    "logger.info(f'Endpoint name: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Deploy FLAN-T5-XL out-of-the-box instruction-tuned model as a SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deployemnt of the model to the endpoint will take around 10-15 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_image_uri = image_uris.retrieve(region=None, \n",
    "                                       framework=None, \n",
    "                                       image_scope=IMAGE_SCOPE, \n",
    "                                       model_id=MODEL_ID, \n",
    "                                       model_version=MODEL_VERSION, \n",
    "                                       instance_type=INSTANCE_TYPE)\n",
    "logger.info(f'Deploy image URI => {deploy_image_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = model_uris.retrieve(model_id=MODEL_ID, \n",
    "                                model_version=MODEL_VERSION, \n",
    "                                model_scope=IMAGE_SCOPE)\n",
    "logger.info(f'Model URI => {model_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT': str(3600),\n",
    "    'MODEL_CACHE_ROOT': '/opt/ml/model', \n",
    "    'SAGEMAKER_ENV': '1',\n",
    "    'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code/',\n",
    "    'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "    'SAGEMAKER_MODEL_SERVER_WORKERS': '1', \n",
    "    'TS_DEFAULT_WORKERS_PER_MODEL': '1',\n",
    "    }\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(image_uri=deploy_image_uri, \n",
    "              model_data=model_uri, \n",
    "              role=ROLE, \n",
    "              predictor_cls=Predictor, \n",
    "              name=endpoint_name, \n",
    "              env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy Model to Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "_ = model.deploy(initial_instance_count=INSTANCE_COUNT, \n",
    "                 instance_type=INSTANCE_TYPE, \n",
    "                 endpoint_name=endpoint_name, \n",
    "                 volume_size=EBS_VOLUME_SIZE, \n",
    "                 model_data_download_timeout=MODEL_DATA_DOWNLOAD_TIMEOUT, \n",
    "                 container_startup_health_check_timeout=CONTAINER_STARTUP_HEALTH_CHECK_TIMEOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the model to be fully deployed from the above code execution before moving to the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Invoke the SageMaker endpoint to test the deployed model for natural language understanding (NLU) and natural language generation (NLG) tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Customer: Hi there, I'm having a problem with my iPhone.\n",
    "Agent: Hi! I'm sorry to hear that. What's happening?\n",
    "Customer: The phone is not charging properly, and the battery seems to be draining very quickly. I've tried different charging cables and power adapters, but the issue persists.\n",
    "Agent: Hmm, that's not good. Let's try some troubleshooting steps. Can you go to Settings, then Battery, and see if there are any apps that are using up a lot of battery life?\n",
    "Customer: Yes, there are some apps that are using up a lot of battery.\n",
    "Agent: Okay, try force quitting those apps by swiping up from the bottom of the screen and then swiping up on the app to close it.\n",
    "Customer: I did that, but the issue is still there.\n",
    "Agent: Alright, let's try resetting your iPhone's settings to their default values. This won't delete any of your data. Go to Settings, then General, then Reset, and then choose Reset All Settings.\n",
    "Customer: Okay, I did that. What's next?\n",
    "Agent: Now, let's try restarting your iPhone. Press and hold the power button until you see the \"slide to power off\" option. Slide to power off, wait a few seconds, and then turn your iPhone back on.\n",
    "Customer: Alright, I restarted it, but it's still not charging properly.\n",
    "Agent: I see. It looks like we need to run a diagnostic test on your iPhone. Please visit the nearest Apple Store or authorized service provider to get your iPhone checked out.\n",
    "Customer: Do I need to make an appointment?\n",
    "Agent: Yes, it's always best to make an appointment beforehand so you don't have to wait in line. You can make an appointment online or by calling the Apple Store or authorized service provider.\n",
    "Customer: Okay, will I have to pay for the repairs?\n",
    "Agent: That depends on whether your iPhone is covered under warranty or not. If it is, you won't have to pay anything. However, if it's not covered under warranty, you will have to pay for the repairs.\n",
    "Customer: How long will it take to get my iPhone back?\n",
    "Agent: It depends on the severity of the issue, but it usually takes 1-2 business days.\n",
    "Customer: Can I track the repair status online?\n",
    "Agent: Yes, you can track the repair status online or by calling the Apple Store or authorized service provider.\n",
    "Customer: Alright, thanks for your help.\n",
    "Agent: No problem, happy to help. Is there anything else I can assist you with?\n",
    "Customer: No, that's all for now.\n",
    "Agent: Alright, have a great day and good luck with your iPhone!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 256\n",
    "NUM_RETURN_SEQUENCES = 1\n",
    "TOP_K = 0\n",
    "TOP_P = 0.7\n",
    "DO_SAMPLE = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'write a summary'\n",
    "prompt = f'{context}\\n{query}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Abstractive Question Answering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What troubleshooting steps were suggested to the customer to fix their iPhone charging issue?'\n",
    "prompt = f'{context}\\n{query}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Was resetting the iPhone to its default settings able to solve the charging issue and battery drain problem?'\n",
    "prompt = f'{context}\\n{query}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What steps can the customer take to make an appointment at the nearest Apple Store or authorized service provider for iPhone repair?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'{context}\\n{query}'\n",
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is the overall sentiment and sentiment score of the conversation between the customer and the agent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'{context}\\n{query}'\n",
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')\n",
    "sentiment = generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Phrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'identify any specific words, phrases, or context that influenced the {sentiment} sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'{context}\\n{query}'\n",
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced features\n",
    "\n",
    "***\n",
    "This model also supports many advanced parameters while performing inference. They include:\n",
    "\n",
    "* **max_length:** Model generates text until the output length (which includes the input context length) reaches `max_length`. If specified, it must be a positive integer.\n",
    "* **num_return_sequences:** Number of output sequences returned. If specified, it must be a positive integer.\n",
    "* **num_beams:** Number of beams used in the greedy search. If specified, it must be integer greater than or equal to `num_return_sequences`.\n",
    "* **no_repeat_ngram_size:** Model ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **early_stopping:** If True, text generation is finished when all beam hypotheses reach the end of sentence token. If specified, it must be boolean.\n",
    "* **do_sample:** If True, sample the next word as per the likelihood. If specified, it must be boolean.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **seed:** Fix the randomized state for reproducibility. If specified, it must be an integer.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint. Next, we show an example of how to invoke endpoint with these arguments\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-shot Learning via In-context Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put in some example input text. You can put any text containing the task, the model returns the output of the accomplished task.\n",
    "\n",
    "Payload must be a json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Zero-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"Once, a cunning fox saw a crow with a piece of cheese in its beak sitting on a branch. \\\n",
    "The fox devised a plan and flattered the crow, causing the crow to caw with delight, dropping the \\\n",
    "cheese which the fox quickly snatched up and ran away. The crow learned a valuable lesson and never trusted the fox again.\"\"\"\n",
    "\n",
    "question = 'who got cheated?'\n",
    "answer = 'crow'\n",
    "\n",
    "prompt = f'Context: {context}<br>Question: {question}<br>Answer:'\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis/Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review = 'I hated the movie. Thoroughly disappointing for a sequel.'\n",
    "# review = 'this product is fine so far'\n",
    "sentiment = 'Sentiment (Good, Bad)'\n",
    "\n",
    "prompt = f'Review: {review}\\n{sentiment}:'\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Inference (NLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "location = \"United States\"\n",
    "# location = \"United Kingdom\"\n",
    "prompt = f\"\"\"The world cup has kicked off in Los Angeles, United States.\n",
    "Based on the paragraph above can we conclude that: ”The world cup takes place in {location}?\"\n",
    "[”yes”, ”no”]\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Title: \"University has new facility coming up\"\n",
    "Given the above title of an imaginary article, imagine the article.\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Translate to German:  My name is Arthur\"\n",
    "\n",
    "logger.info(f\"Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Customer: Hi there, I'm having a problem with my iPhone.\n",
    "Agent: Hi! I'm sorry to hear that. What's happening?\n",
    "Customer: The phone is not charging properly, and the battery seems to be draining very quickly. I've tried different charging cables and power adapters, but the issue persists.\n",
    "Agent: Hmm, that's not good. Let's try some troubleshooting steps. Can you go to Settings, then Battery, and see if there are any apps that are using up a lot of battery life?\n",
    "Customer: Yes, there are some apps that are using up a lot of battery.\n",
    "Agent: Okay, try force quitting those apps by swiping up from the bottom of the screen and then swiping up on the app to close it.\n",
    "Customer: I did that, but the issue is still there.\n",
    "Agent: Alright, let's try resetting your iPhone's settings to their default values. This won't delete any of your data. Go to Settings, then General, then Reset, and then choose Reset All Settings.\n",
    "Customer: Okay, I did that. What's next?\n",
    "Agent: Now, let's try restarting your iPhone. Press and hold the power button until you see the \"slide to power off\" option. Slide to power off, wait a few seconds, and then turn your iPhone back on.\n",
    "Customer: Alright, I restarted it, but it's still not charging properly.\n",
    "Agent: I see. It looks like we need to run a diagnostic test on your iPhone. Please visit the nearest Apple Store or authorized service provider to get your iPhone checked out.\n",
    "Customer: Do I need to make an appointment?\n",
    "Agent: Yes, it's always best to make an appointment beforehand so you don't have to wait in line. You can make an appointment online or by calling the Apple Store or authorized service provider.\n",
    "Customer: Okay, will I have to pay for the repairs?\n",
    "Agent: That depends on whether your iPhone is covered under warranty or not. If it is, you won't have to pay anything. However, if it's not covered under warranty, you will have to pay for the repairs.\n",
    "Customer: How long will it take to get my iPhone back?\n",
    "Agent: It depends on the severity of the issue, but it usually takes 1-2 business days.\n",
    "Customer: Can I track the repair status online?\n",
    "Agent: Yes, you can track the repair status online or by calling the Apple Store or authorized service provider.\n",
    "Customer: Alright, thanks for your help.\n",
    "Agent: No problem, happy to help. Is there anything else I can assist you with?\n",
    "Customer: No, that's all for now.\n",
    "Agent: Alright, have a great day and good luck with your iPhone!\n",
    "\"\"\"\n",
    "\n",
    "query = (\n",
    "    \"What are the customer and agent talking about?\"\n",
    ")\n",
    "\n",
    "# \"What troubleshooting steps were suggested to the customer to fix their iPhone charging issue?\"\n",
    "# \"Was resetting the iPhone to its default settings able to solve the charging issue and battery draining?\"\n",
    "# \"What steps can the customer take to make an appointment at the nearest Apple Store or authorized service provider?\"\n",
    "# \"What is the overall sentiment and sentiment score of the conversation between the customer and the agent?\"\n",
    "# \"identify any specific words, phrases, or context that influenced the {sentiment} sentiment.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Context Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_context = \"\"\"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) \\\n",
    "were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. \\\n",
    "They were descended from Norse (\\\"Norman\\\" comes from \\\"Norseman\\\") raiders and pirates from Denmark, \\\n",
    "Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. \\\n",
    "Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, \\\n",
    "their descendants would gradually merge with the Carolingian-based cultures of West Francia. \\\n",
    "The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, \\\n",
    "and it continued to evolve over the succeeding centuries.\"\"\"\n",
    "test_question = \"In what country is Normandy located?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context: \n",
    "{test_context}\n",
    "\n",
    "{test_question}\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B. One-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_article = 'I love apples especially the large juicy ones. Apples are a great source of vitamins and fiber. An apple a day keeps the doctor away!'\n",
    "train_summary = 'I love apples.'\n",
    "\n",
    "test_article = 'I hate oranges especially the bitter ones. They are high in citric acid and they give me heart burns. Doctor suggests me to avoid them!'\n",
    "test_summary = 'I hate oranges.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "article: {train_article}\n",
    "summary: {train_summary}\n",
    "--\n",
    "article: {test_article}\n",
    "summary:\"\"\"\n",
    "\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Generation (NLG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inp = 'name[The Punter], eat_type[Indian], price_range[cheap]'\n",
    "train_out = 'The Punter provides Indian food in the cheap price range.'\n",
    "\n",
    "test_inp = 'name[Blue Spice], eatType[coffee shop], price_range[expensive]'\n",
    "test_out = 'Blue Spice is a coffee shop that is a bit expensive.'\n",
    "\n",
    "prompt = (\n",
    "    f\"{train_inp} ==> \"\n",
    "    f\"sentence describing the place: {train_out} ; \"\n",
    "    f\"{test_inp} ==> sentence describing the place:\"\n",
    ")\n",
    "\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flip (Entity Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inp = 'The Punter provides Indian food in the cheap price range.'\n",
    "train_out = 'name[The Punter], eat_type[Indian], price_range[cheap]'\n",
    "\n",
    "test_inp = 'Blue Spice is a coffee shop that is a bit pricy.'\n",
    "test_out = 'name[Blue Spice], eat_type[coffee shop], price_range[pricy]'\n",
    "\n",
    "prompt = (\n",
    "    f\"\"\"{train_inp} ==> {train_out}\n",
    "--\n",
    "{test_inp} ==>\"\"\"\n",
    ")\n",
    "\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Few-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Translate English to French:\n",
    "\n",
    "sea otter => loutre de mer\n",
    "--\n",
    "peppermint => menthe poivrée\n",
    "--\n",
    "plush girafe => girafe en peluche\n",
    "--\n",
    "cheese =>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Extract main person:\n",
    "\n",
    "s: John is playing basketball\n",
    "p: John\n",
    "##\n",
    "s: Jeff and Phil are chatting about GAI. Phil has to run. He is in a rush\n",
    "p: Phil\n",
    "##\n",
    "s: Max is older than Emma\n",
    "p: Max\n",
    "##\n",
    "s: Susan misses the bus this morning but still get in time for her meeting with Sara\n",
    "p:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Classify the topic of the following paragraph: Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group, which has a reputation for making well-timed and occasionally controversial plays in the defense industry, has quietly placed its bets on another part of the market.\n",
    "Label: Business.\n",
    "\n",
    "##\n",
    "\n",
    "Classify the topic of the following paragraph: Some People Not Eligible to Get in on Google IPO Google has billed its IPO as a way for everyday people to get in on the process, denying Wall Street the usual stranglehold it's had on IPOs. Public bidding, a minimum of just five shares, an open process with 28 underwriters - all this pointed to a new level of public participation. But this isn't the case.\n",
    "Label: Technology.\n",
    "\n",
    "##\n",
    "\n",
    "Classify the topic of the following paragraph: Indians Mount Charge The Cleveland Indians pulled within one game of the AL Central lead by beating the Minnesota Twins, 7-1, Saturday night with home runs by Travis Hafner and Victor Martinez.\n",
    "Label: Sports.\n",
    "\n",
    "##\n",
    "\n",
    "Classify the topic of the following paragraph: Uptown girl, she's been living in her uptown world, I bet she never had a backstreet guy, I bet her mother never told her why, I'm gonna try.\n",
    "Label:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: My native language is not English, I have blogs and I write my own articles. I also get articles from outsource writers, so I want to use it to re-write such articles, so i will create a tool for that\n",
    "NLP Task: Paraphrasing\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: Just experimenting with content generation\n",
    "NLP task: Long form generation\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: My company MetaDialog provides human in the loop automated support for costumers, we are currently using GPT3 to generate answers using our custom search engine to clients questions, and then provide the answers as suggestions to human agents to use or reject them as real answers to clients.\n",
    "NLP task: Conversational agent\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: Receipt extraction including line items from plain text (sources being OCR-ed images, PDFs and HTML Emails)\n",
    "NLP task: Information extraction\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: I have a lot of legacy documentation which needs to be cleaned, summarized, and queried. I think AI21 would help.\n",
    "NLP task: Summarization\n",
    "\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: Answer questions based on a given corpus of information\n",
    "NLP task: Question answering\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: creating useful content for companies websites articles\n",
    "NLP task:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
